{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b736e77a-4a52-47f9-b310-941825305abe",
   "metadata": {},
   "source": [
    "# One-time Prerequisites:\n",
    "Install azure blob storage and google auth packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998de4fc-27e9-442e-a86e-123c1a787901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install azure-storage-blob==12.20.0\n",
    "# %pip install google-auth\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a797a3-451c-44d5-8670-e92f6bb7b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobClient\n",
    "\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request as GoogleAuthRequest\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8ce75-b6c4-4d76-806c-a7cf10d5aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSPS_URL = \"https://tsps.dsde-dev.broadinstitute.org\"\n",
    "IMPUTATION_BEAGLE_VERSION = \"0.0.1\"\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"Get access token.\"\"\"\n",
    "\n",
    "    # scopes = [\"https://www.googleapis.com/auth/userinfo.profile\", \"https://www.googleapis.com/auth/userinfo.email\"]\n",
    "    # credentials = GoogleCredentials.get_application_default()\n",
    "    # credentials = credentials.create_scoped(scopes)\n",
    "\n",
    "    # return credentials.get_access_token().access_token\n",
    "\n",
    "    credentials, _ = google.auth.default()\n",
    "    \n",
    "    credentials.refresh(GoogleAuthRequest())\n",
    "    \n",
    "    return credentials.token\n",
    "\n",
    "def prepare_imputation_pipeline(multi_sample_vcf_path, output_basename, token):\n",
    "    request_body = {\n",
    "        \"jobId\": f\"{uuid.uuid4()}\",\n",
    "        \"pipelineVersion\": \"string\",\n",
    "        \"pipelineInputs\": {\n",
    "            \"multiSampleVcf\": multi_sample_vcf_path,\n",
    "            \"outputBasename\": output_basename\n",
    "        }\n",
    "    }\n",
    "\n",
    "    uri = f\"{TSPS_URL}/api/pipelineruns/v1/imputation_beagle/prepare\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(uri, json=request_body, headers=headers)\n",
    "    status_code = response.status_code\n",
    "\n",
    "    if status_code != 200:\n",
    "        raise Exception(response.text)\n",
    "\n",
    "    response = json.loads(response.text)\n",
    "    job_id = response['jobId']\n",
    "\n",
    "    print(f\"Successfully prepared imputation pipeline run with job_id {job_id}\")\n",
    "\n",
    "    return job_id, response['fileInputUploadUrls']\n",
    "\n",
    "# run imputation beagle pipeline\n",
    "def start_imputation_pipeline(job_id, description, token):\n",
    "    request_body = {\n",
    "        \"description\": description,\n",
    "        \"jobControl\": {\n",
    "            \"id\": job_id\n",
    "        }\n",
    "    }\n",
    "\n",
    "    uri = f\"{TSPS_URL}/api/pipelineruns/v1/imputation_beagle/start\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(uri, json=request_body, headers=headers)\n",
    "    status_code = response.status_code\n",
    "\n",
    "    if status_code != 202:\n",
    "        raise Exception(response.text)\n",
    "\n",
    "    print(f\"Successfully started imputation pipeline run for job_id {job_id}\")\n",
    "    return\n",
    "\n",
    "\n",
    "# poll for imputation beagle job; if successful, return the pipelineOutput object (dict)\n",
    "def check_imputation_job_status(job_id, token):\n",
    "    uri = f\"{TSPS_URL}/api/pipelineruns/v1/imputation_beagle/result/{job_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(uri, headers=headers)\n",
    "    status_code = response.status_code\n",
    "    response = json.loads(response.text)\n",
    "\n",
    "    if status_code == 200:\n",
    "        # job is completed, test for status\n",
    "        if response['jobReport']['status'] == 'SUCCEEDED':\n",
    "            print(f\"pipeline has succeeded: {response}\")\n",
    "            # return the pipeline output dictionary\n",
    "            return response['jobReport']['status'], response['pipelineOutput']\n",
    "        else:\n",
    "            return response['jobReport']['status'], response['errorReport']\n",
    "    elif status_code == 202:\n",
    "        print(\"tsps pipeline still running\")\n",
    "        return response['jobReport']['status'], None\n",
    "        \n",
    "    else:\n",
    "        raise Exception(f'pipeline failed with a {status_code} status code. has response {response.text}')\n",
    "\n",
    "\n",
    "def sizeof_fmt(num, suffix=\"B\"):\n",
    "    for unit in (\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"):\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f}{unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.1f}Yi{suffix}\"\n",
    "    \n",
    "\n",
    "def upload_file_with_azcopy(local_file_path, write_sas_url):\n",
    "    blob_client = BlobClient.from_blob_url(write_sas_url)\n",
    "    file_size_bytes = os.path.getsize(local_file_path)\n",
    "    file_size_human_readable = sizeof_fmt(file_size_bytes)\n",
    "\n",
    "    print(f\"uploading file from {local_file_path}, file size: {file_size_human_readable} \\n\")\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    def upload_progress_report(response):\n",
    "        current = response.context['upload_stream_current']  #There's also a 'download_stream_current'\n",
    "        total = response.context['data_stream_total']\n",
    "\n",
    "        if current is None:\n",
    "            current = total\n",
    "            \n",
    "        percent_done = round(100*current/total, 1)\n",
    "        duration_m = round((time.time() - start) / 60, 1)\n",
    "        print(f\"uploaded {sizeof_fmt(current)} out of {sizeof_fmt(total)} total ({percent_done}%, {duration_m} min elapsed) \\t\\t\\t\", end='\\r')\n",
    "    \n",
    "    # upload the file\n",
    "    with open(file=local_file_path, mode=\"rb\") as blob_file:\n",
    "        blob_client.upload_blob(blob_file, max_concurrency=8, raw_response_hook=upload_progress_report)\n",
    "\n",
    "\n",
    "def download_with_azcopy(read_sas_url, local_file_path=None):\n",
    "    blob_client = BlobClient.from_blob_url(read_sas_url)\n",
    "\n",
    "    if local_file_path == None:\n",
    "        # extract the file name from the sas url\n",
    "        local_file_path = read_sas_url.split(\"?\")[0].split(\"/\")[-1] \n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    def download_progress_report(response):\n",
    "        current = response.context['download_stream_current']\n",
    "        total = response.context['data_stream_total']\n",
    "        if current is not None:\n",
    "            percent_done = round(100*current/total, 1)\n",
    "            duration_m = round((time.time() - start) / 60, 1)\n",
    "            print(f\"downloaded {sizeof_fmt(current)} out of {sizeof_fmt(total)} total ({percent_done}%, {duration_m} min elapsed) \\t\\t\\t\", end='\\r')\n",
    "    \n",
    "    print(f\"downloading file to {local_file_path} \\n\")\n",
    "\n",
    "    # download the file\n",
    "    with open(file=local_file_path, mode=\"wb\") as blob_file:\n",
    "        download_stream = blob_client.download_blob(max_concurrency=8, raw_response_hook=download_progress_report)\n",
    "        blob_file.write(download_stream.readall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17243239-1508-4169-80d9-fd81f018ca36",
   "metadata": {},
   "source": [
    "## Prepare your imputation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f92b9b-04b0-44bd-99c4-5ef3ba94c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_input_files = {\n",
    "    \"multiSampleVcf\": \"palantir_merged_input_samples.liftedover.vcf.gz\"\n",
    "}\n",
    "output_basename = \"palantir_merged_samples\"\n",
    "description = \"notebook run to dev 5 from indiana - added maxRetries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25395a4-501c-4631-8276-273c7a614614",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sample_vcf_path = local_input_files[\"multiSampleVcf\"]\n",
    "\n",
    "job_id, file_input_upload_urls = prepare_imputation_pipeline(multi_sample_vcf_path, output_basename, get_access_token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ef90e-988d-4ba4-8b3d-dc4cadbf36c1",
   "metadata": {},
   "source": [
    "## Choose one of the following two methods to upload your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e4a77-d5e3-4d41-aa1c-fc24d2b72f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_method = \"notebook\"     # upload through jupyter notebook\n",
    "# upload_method = \"command line\" # upload manually via your computer's command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4ab4d-40f1-4f3a-a1aa-3aadee8fb3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for input_file_key, sas_info in file_input_upload_urls.items():\n",
    "\n",
    "    if upload_method == \"notebook\":\n",
    "        upload_file_with_azcopy(local_input_files[input_file_key], sas_info[\"sasUrl\"])\n",
    "    \n",
    "    elif upload_method == \"command line\":\n",
    "        print(f\"command to upload {input_file_key}:\\n\")\n",
    "        print(sas_info['azcopyCommand'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb46d2c-a55e-46ab-86b5-ef2f7b1bd537",
   "metadata": {},
   "source": [
    "## Start your prepared imputation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e745092-df8f-4560-b31c-43d909279dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_imputation_pipeline(job_id, description, get_access_token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6de3a-d676-4e83-9fbc-2a24254018aa",
   "metadata": {},
   "source": [
    "## Check pipeline run status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644cb0f-526e-49c2-a325-f8c574129b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, output = check_imputation_job_status(job_id, get_access_token())\n",
    "print(response)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcb54e-82d7-46c5-9149-6bf8c66c4471",
   "metadata": {},
   "source": [
    "## Once pipeline run has status SUCCEEDED, retrieve your outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fd82c-96bb-4c93-aa79-e887741228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_method = \"notebook\"\n",
    "# download_method = \"command line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc4983-15ab-4a78-ae03-cd1584b93ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_file_key, read_sas_url in output.items():\n",
    "\n",
    "    if download_method == \"notebook\":\n",
    "        download_with_azcopy(read_sas_url)\n",
    "\n",
    "    elif download_method == \"command line\":\n",
    "        print(f\"command to download {output_file_key}:\\n\")\n",
    "        print(f\"azcopy copy {read_sas_url} . \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
